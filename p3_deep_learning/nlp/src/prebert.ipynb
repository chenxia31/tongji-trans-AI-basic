{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from util import load_data\n",
    "data_path='../data/sst2_shuffled.tsv.1'\n",
    "train_data,test_data,categories=load_data.load_sentence_polarity(data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里新的模型transformer\n",
    "# 先确定pre train模型的名称，所确定的tokenize\n",
    "# 加载预训练模型，因为这里是英文数据集，需要用在英文上的预训练模型：bert-base-uncased\n",
    "# uncased指该预训练模型对应的词表不区分字母的大小写\n",
    "# 详情可了解：https://huggingface.co/bert-base-uncased\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from transformers import BertTokenizer\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写好制作数据集的方式，先定义dataset、后面定义dataloader\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset=dataset\n",
    "        self.data_size=len(dataset)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "def coffate_fn(examples):\n",
    "    inputs,targets=[],[]\n",
    "    for polar,sent in examples:\n",
    "        inputs.append(sent)\n",
    "        targets.append(int(polar))\n",
    "    # 这里的tokenizer是后面提供好pretrain model之后的API\n",
    "    inputs = tokenizer(inputs,\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                       max_length=512)\n",
    "    targets = torch.tensor(targets)\n",
    "    return inputs,targets\n",
    "\n",
    "pretrained_model_name = 'bert-base-uncased'\n",
    "# 加载预训练模型对应的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "batch_size=32\n",
    "train_dataset=BertDataset(train_data)\n",
    "test_dataset=BertDataset(test_data)\n",
    "train_dataloader=DataLoader(train_dataset,batch_size=batch_size,collate_fn=coffate_fn,shuffle=True)\n",
    "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,collate_fn=coffate_fn,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后是定义模型的名称\n",
    "class BertSST2Model(nn.Module):\n",
    "    def __init__(self,class_size,pretrained_model_name=pretrained_model_name) -> None:\n",
    "        super(BertSST2Model,self).__init__()\n",
    "        \"\"\"\n",
    "        NOTE 这里是需要编程的地方\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        前向推理的过程\n",
    "        inputs 处理好的数据 shape=batchsize*max_len\n",
    "        \"\"\"\n",
    "        input_ids,input_tyi,input_attn_mask=inputs['input_ids'],inputs['token_type_ids'],inputs['attention_mask']\n",
    "        \"\"\"\n",
    "        NOTE 这里是需要编程的地方\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "def save_pretrained(model, path):\n",
    "    # 保存模型，先利用os模块创建文件夹，后利用torch.save()写入模型文件\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(model, os.path.join(path, 'model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=64\n",
    "num_epoch=200\n",
    "check_step=20\n",
    "learning_rate=1e-5\n",
    "model=BertSST2Model(class_size=2)\n",
    "model.to(device)\n",
    "optimizer=Adam(model.parameters(),learning_rate)\n",
    "celoss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 记录当前训练时间，用以记录日志和存储\n",
    "timestamp = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.train()\n",
    "for epoch in range(1,num_epoch+1):\n",
    "    total_loss=0\n",
    "    # 训练过程\n",
    "    for batch in tqdm(train_dataloader,desc=f'Training epoch {epoch}'):\n",
    "        inputs,targets=[x.to(device) for x in batch]\n",
    "        \"\"\"\n",
    "        NOTE 修改你的代码完成模型训练\n",
    "        \"\"\"\n",
    "    #测试过程\n",
    "    # acc统计模型在测试数据上分类结果中的正确个数\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_dataloader, desc=f\"Testing\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        # with torch.no_grad(): 为固定写法，\n",
    "        # 这个代码块中的全部有关tensor的操作都不产生梯度。目的是节省时间和空间，不加也没事\n",
    "        with torch.no_grad():\n",
    "            bert_output = model(inputs)\n",
    "            \"\"\"\n",
    "            .argmax()用于取出一个tensor向量中的最大值对应的下表序号，dim指定了维度\n",
    "            假设 bert_output为3*2的tensor：\n",
    "            tensor\n",
    "            [\n",
    "                [3.2,1.1],\n",
    "                [0.4,0.6],\n",
    "                [-0.1,0.2]\n",
    "            ]\n",
    "            则 bert_output.argmax(dim=1) 的结果为：tensor[0,1,1]\n",
    "            \"\"\"\n",
    "            acc += (bert_output.argmax(dim=1) == targets).sum().item()\n",
    "    #输出在测试集上的准确率\n",
    "    print(f\"Acc: {acc / len(test_dataloader):.2f}\")\n",
    "    if epoch % check_step == 0:\n",
    "        \"\"\"\n",
    "        NOTE 定期保存你的代码\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记录checkpoint,之后load最新的参数\n",
    "# model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "model.eval()\n",
    "test='why teacher shen are so unhappy ?'\n",
    "test=tokenizer(test,padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512)\n",
    "test.to(device)\n",
    "if model(test).argmax(-1).item()==1:\n",
    "    print('This is a negative sentence')\n",
    "else:\n",
    "    print('This is a positive sentence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ca69f3d02436474f504e5bc8aa3d57c6bfabac6fe00c9a130f4089cd6bf168"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
